# Self-supervised learning and unsupervised learning: a linear case analysis
With the exponentially increasing cost of labelling, it is a long-lasting question about how to summarize and make good use of those data. Self-supervised learning, which learns from the representation of unlabeled data, motivates such techniques significantly.

Despite the progress, it remains a significant research challenge about finding reasons for the success of self-supervised learning and the fundamental difference between self-supervised learning and unsupervised learning. The research will mainly focus on the math derivation of linear case analysis of those algorithms. Specifically, linear autoencoder and masked linear autoencoder will be examined as the representation of unsupervised learning and self-supervised learning in the simulation of a regression example. As a result, even in simple linear cases, the regression loss of certain types of masked autoencoders can be lower than that of a linear autoencoder in certain circumstances, which indeed shows the merit of self-supervised learning. Such fundamental research will demonstrate the stability and adaptability of the algorithm, which will motivate other researchers to build on a wider and deeper study of self-supervised learning.
