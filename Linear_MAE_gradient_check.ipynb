{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Verify the gradient derivation of linear masked autoencoder.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise matrix and vector\n",
    "m = 50\n",
    "n = 10\n",
    "prob = 0.6\n",
    "X = torch.rand(m, n)\n",
    "W1 = torch.rand(n, n, requires_grad=True)\n",
    "W2 = torch.rand(n, n, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different types of masks\n",
    "def mask_basic(prob, m, n):\n",
    "    return torch.zeros(m, n).bernoulli_(prob)\n",
    "\n",
    "def mask_dropping_probs(prob_list: torch.Tensor, m, n):\n",
    "    return torch.zeros(m, n).bernoulli_(prob_list)\n",
    "\n",
    "def mask_patches(prob, patch_size, m, n):\n",
    "    if not n % patch_size:\n",
    "        pix_num = n // patch_size\n",
    "        mat_patches = torch.zeros(m, pix_num).bernoulli_(prob)\n",
    "        return mat_patches.repeat_interleave(patch_size, dim=1)\n",
    "    else:\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_basic(W1):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_basic(W2):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2928.7549, 2423.7131, 3062.7380, 2925.0562, 2736.2166, 2715.0981,\n",
       "         3384.3022, 2748.9661, 2966.3354, 3231.7920],\n",
       "        [2167.0989, 1778.6750, 2264.7625, 2161.1702, 2015.5416, 1990.7881,\n",
       "         2496.6406, 2036.9464, 2189.6941, 2373.5215],\n",
       "        [2415.8381, 1988.2748, 2549.7305, 2422.0857, 2263.9260, 2229.2139,\n",
       "         2812.9221, 2285.7661, 2450.0088, 2674.1602],\n",
       "        [3371.6895, 2769.1604, 3539.1252, 3373.8645, 3136.0845, 3125.5830,\n",
       "         3912.2446, 3169.8306, 3409.8096, 3734.4739],\n",
       "        [3059.6621, 2527.8501, 3202.0374, 3057.0481, 2868.6990, 2810.4558,\n",
       "         3517.7339, 2880.7788, 3097.1609, 3357.7683],\n",
       "        [2402.6104, 1984.2102, 2527.0803, 2401.4229, 2239.8945, 2238.6538,\n",
       "         2805.3030, 2263.0488, 2441.6416, 2667.9233],\n",
       "        [2034.5262, 1676.0745, 2123.3013, 2029.9707, 1896.3358, 1872.0820,\n",
       "         2334.2024, 1908.7225, 2055.7942, 2236.1763],\n",
       "        [2345.4126, 1942.9313, 2443.9529, 2342.9968, 2190.1743, 2162.2864,\n",
       "         2695.7705, 2195.5542, 2362.4333, 2579.3604],\n",
       "        [3158.1003, 2603.4167, 3299.0784, 3152.7900, 2948.5427, 2909.4155,\n",
       "         3633.5027, 2965.6555, 3189.5605, 3465.1128],\n",
       "        [2666.3682, 2190.7024, 2798.7822, 2668.7932, 2478.7410, 2469.2678,\n",
       "         3095.1538, 2506.6426, 2694.9556, 2956.2417]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the theoretical and numerical solutions of W1 and W2\n",
    "# mask_basic\n",
    "mean_m_basic = torch.ones(m, n) * prob\n",
    "square_m_basic = torch.ones(n, n) * prob**2\n",
    "square_m_basic.fill_diagonal_(prob)\n",
    "\n",
    "grad_w1_theory_basic = W2.T @ (W2@W1@(square_m_basic*(X.T@X)) - X.T@(mean_m_basic*X)) * (2/m/n)\n",
    "grad_w2_theory_basic = (W2@W1@(square_m_basic*(X.T@X))-X.T@(mean_m_basic*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 1000\n",
    "grad_w1_numer_basic = 0\n",
    "grad_w2_numer_basic = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_basic += torch.autograd.functional.jacobian(loss_func_W1_basic, W1)\n",
    "    grad_w2_numer_basic += torch.autograd.functional.jacobian(loss_func_W2_basic, W2)\n",
    "    W1.detach()\n",
    "    W2.detach()\n",
    "grad_w1_numer_basic / N\n",
    "grad_w2_numer_basic / N\n",
    "\n",
    "grad_w1_numer_basic\n",
    "#norm_diff_w1 = torch.linalg.matrix_norm(grad_w1-grad_w1_true)\n",
    "#norm_diff_w2 = torch.linalg.matrix_norm(grad_w2-grad_w2_true)\n",
    "\n",
    "#print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1.item())\n",
    "#print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_probs(W1):\n",
    "    z = (mask_dropping_probs(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_probs(W2):\n",
    "    z = (mask_dropping_probs(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2940.8464, 2433.2715, 3076.4978, 2943.4871, 2774.6506, 2761.4392,\n",
       "         3396.5461, 2729.7273, 2978.4912, 3248.4111],\n",
       "        [2175.8208, 1785.6799, 2274.4841, 2174.8479, 2043.7139, 2024.8901,\n",
       "         2505.4998, 2022.4923, 2198.7373, 2385.6819],\n",
       "        [2425.6426, 1995.9534, 2560.5955, 2437.2378, 2295.8110, 2267.5999,\n",
       "         2822.8616, 2269.2683, 2460.1506, 2687.5793],\n",
       "        [3385.2117, 2780.2659, 3554.6824, 3394.9675, 3180.3481, 3179.1077,\n",
       "         3926.0044, 3147.3542, 3423.8721, 3753.2332],\n",
       "        [3072.4275, 2537.5488, 3215.7354, 3076.3008, 2908.6423, 2858.6912,\n",
       "         3530.5283, 2860.4102, 3109.9451, 3375.1941],\n",
       "        [2412.2893, 1992.0916, 2538.6189, 2416.5273, 2271.6750, 2276.9592,\n",
       "         2815.2249, 2247.0300, 2451.6768, 2681.3105],\n",
       "        [2042.8871, 1682.6747, 2132.5168, 2042.7648, 1922.7485, 1904.0958,\n",
       "         2342.5352, 1895.3468, 2064.2466, 2247.7844],\n",
       "        [2355.2891, 1950.6976, 2454.7852, 2357.8203, 2220.8254, 2199.2422,\n",
       "         2705.7156, 2180.2129, 2372.1641, 2592.7654],\n",
       "        [3171.1804, 2613.6721, 3313.4351, 3172.6414, 2989.6990, 2959.1631,\n",
       "         3646.7815, 2944.7771, 3202.7214, 3483.1860],\n",
       "        [2677.0583, 2199.5193, 2811.0718, 2685.5273, 2513.7773, 2511.6074,\n",
       "         3105.9790, 2488.8320, 2706.0845, 2970.9846]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_dropping_probs\n",
    "prob_list = torch.rand(n)\n",
    "mean_m_probs = prob_list.repeat(m, 1)\n",
    "square_m_probs = prob_list.view(n, 1) @ prob_list.view(1, n)\n",
    "square_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\n",
    "\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X)) * (2/m/n)\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 1000\n",
    "grad_w1_numer_probs = 0\n",
    "grad_w2_numer_probs = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_probs += torch.autograd.functional.jacobian(loss_func_W1_probs, W1)\n",
    "    grad_w2_numer_probs += torch.autograd.functional.jacobian(loss_func_W2_probs, W2)\n",
    "grad_w1_numer_probs / N\n",
    "grad_w2_numer_probs / N\n",
    "\n",
    "abs(grad_w1_theory_probs-grad_w1_numer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_patches(W1):\n",
    "    z = (mask_patches(prob, patch_size, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_patches(W2):\n",
    "    z = (mask_patches(prob, patch_size, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3055.3391, 2584.1592, 3207.7812, 3145.1729, 2969.8660, 2936.4583,\n",
       "         3567.0972, 2904.6987, 3131.7996, 3385.6895],\n",
       "        [2255.5510, 1899.5037, 2372.1047, 2327.7117, 2185.0222, 2153.2278,\n",
       "         2634.0022, 2153.7358, 2307.0090, 2483.9072],\n",
       "        [2511.2561, 2119.6418, 2673.9436, 2611.7866, 2448.3037, 2410.2981,\n",
       "         2967.5339, 2419.1169, 2591.7263, 2800.4080],\n",
       "        [3506.1997, 2953.7954, 3708.0137, 3634.6118, 3403.2258, 3372.7830,\n",
       "         4120.5054, 3355.3027, 3606.8003, 3905.8286],\n",
       "        [3190.8740, 2699.5879, 3356.3948, 3290.2515, 3104.3398, 3047.8022,\n",
       "         3716.5864, 3037.8496, 3265.4751, 3517.6233],\n",
       "        [2500.7712, 2112.6472, 2646.3867, 2586.7690, 2433.3599, 2413.7000,\n",
       "         2952.4780, 2400.4111, 2582.7148, 2795.5845],\n",
       "        [2121.7808, 1790.3220, 2224.9912, 2182.6672, 2055.8218, 2026.7932,\n",
       "         2464.1260, 2012.4594, 2168.9951, 2341.5400],\n",
       "        [2449.1279, 2074.3503, 2560.9006, 2518.9949, 2375.2085, 2343.0017,\n",
       "         2842.0945, 2315.4304, 2493.6509, 2696.7095],\n",
       "        [3294.0120, 2779.3442, 3456.2810, 3393.2849, 3197.2800, 3151.7041,\n",
       "         3834.4497, 3128.8142, 3360.9417, 3624.6323],\n",
       "        [2771.8054, 2337.1851, 2932.8645, 2875.3213, 2689.0020, 2663.7957,\n",
       "         3259.1484, 2654.2839, 2853.1030, 3091.6277]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_patches\n",
    "patch_size = 2\n",
    "mean_m_patches = torch.ones(m, n) * prob\n",
    "square_m_patches = torch.ones(n, n) * prob**2\n",
    "for i in range(n):\n",
    "    place_val = i // patch_size\n",
    "    square_m_patches[place_val:place_val+patch_size, place_val:place_val+patch_size] = torch.ones(patch_size, patch_size) * prob\n",
    "\n",
    "grad_w1_theory_patches = W2.T @ (W2@W1@(square_m_patches*(X.T@X)) - X.T@(mean_m_patches*X)) * (2/m/n)\n",
    "grad_w2_theory_patches = (W2@W1@(square_m_patches*(X.T@X))-X.T@(mean_m_patches*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 1000\n",
    "grad_w1_numer_patches = 0\n",
    "grad_w2_numer_patches = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_patches += torch.autograd.functional.jacobian(loss_func_W1_patches, W1)\n",
    "    grad_w2_numer_patches += torch.autograd.functional.jacobian(loss_func_W2_patches, W2)\n",
    "grad_w1_numer_patches / N\n",
    "grad_w2_numer_patches / N\n",
    "\n",
    "abs(grad_w1_theory_patches-grad_w1_numer_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('SSL_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c3c12edae13c0508bae753e64eeaec91e16c05e32c8c48bac5bd7ee2e37cd7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
