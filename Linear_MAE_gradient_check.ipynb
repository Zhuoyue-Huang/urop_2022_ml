{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Verify the gradient derivation of linear masked autoencoder.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise matrix and vector\n",
    "# prob is un-masking prob\n",
    "sample_num = 3\n",
    "H = 4\n",
    "W = 1\n",
    "sample_dim = [H, W]\n",
    "feature_num = H * W\n",
    "\n",
    "m = sample_num\n",
    "n = feature_num\n",
    "p = feature_num // 2\n",
    "prob = 0.75\n",
    "X = torch.rand(m, n)\n",
    "W1 = torch.rand(p, n, requires_grad=True)\n",
    "W2 = torch.rand(n, p, requires_grad=True)\n",
    "#print(X)\n",
    "#print(W1)\n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different types of masks\n",
    "def mask_basic(prob, sample_num, feature_num):\n",
    "    return torch.zeros(sample_num, feature_num).bernoulli_(prob)\n",
    "\n",
    "def mask_dropping_probs(prob_list: torch.Tensor, sample_num, feature_num):\n",
    "    return torch.zeros(sample_num, feature_num).bernoulli_(prob_list)\n",
    "\n",
    "def mask_patches(prob, patch_size, sample_num, sample_dim):\n",
    "    patch_size = torch.tensor(patch_size)\n",
    "    sample_dim = torch.tensor(sample_dim)\n",
    "    feature_num = sample_dim[0]*sample_dim[1]\n",
    "    div_check = sample_dim % patch_size == torch.zeros(2)\n",
    "    if torch.all(div_check):\n",
    "        pix_num = torch.div(sample_dim, patch_size, rounding_mode='floor')\n",
    "        mat_patches = torch.zeros(sample_num, *pix_num).bernoulli_(prob)\n",
    "        mat_patches = torch.repeat_interleave(mat_patches, patch_size[1], dim=2)\n",
    "        return mat_patches.repeat_interleave(patch_size[0], dim=1).view(sample_num, feature_num)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Both height ({H}) and width ({W}) should be divisible by patch_size ({patch_size}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked autoencoder (linear)\n",
    "class M_LAE(nn.Module):\n",
    "    def __init__(self, prob, sample_num, sample_dim, reduction_dim, type='basic', patch_size=None):\n",
    "        super(M_LAE, self).__init__()\n",
    "        self.prob = prob\n",
    "        self.m = sample_num\n",
    "        self.sample_dim = sample_dim\n",
    "        self.H, self.W = sample_dim\n",
    "        self.n = self.H * self.W\n",
    "        self.p = reduction_dim\n",
    "        if type not in ['basic', 'probs', 'patches']:\n",
    "            raise NotImplementedError(\"Could only implement 'basic', 'probs' and 'patches' type of masking.\")\n",
    "        else:\n",
    "            self.masking_type = type\n",
    "        if patch_size is not None:\n",
    "            self.patch_size = patch_size\n",
    "        w1 = nn.Linear(self.n, self.p, bias=False)\n",
    "        w2 = nn.Linear(self.p, self.n, bias=False)\n",
    "        self.body = nn.Sequential(*[w1, w2])\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        if mask is None:\n",
    "            if self.masking_type == 'basic':\n",
    "                mask = mask_basic(self.prob, self.m, self.n)\n",
    "            elif self.masking_type == 'probs':\n",
    "                mask = mask_dropping_probs(self.prob, self.m, self.n)\n",
    "            elif self.masking_type == 'patches':\n",
    "                mask = mask_patches(self.prob, self.patch_size, self.m, self.sample_dim)\n",
    "        Y = mask * X\n",
    "        Y = self.body(Y)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_basic(W1):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_basic(W2):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.8443e-04, 7.4720e-04, 1.0729e-06, 6.1157e-04],\n",
       "        [5.2887e-04, 9.8798e-04, 3.3244e-05, 9.1496e-04]],\n",
       "       grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the theoretical and numerical solutions of W1 and W2\n",
    "# mask_basic\n",
    "mean_m_basic = torch.ones(m, n) * prob\n",
    "square_m_basic = torch.ones(n, n) * prob**2\n",
    "square_m_basic.fill_diagonal_(prob)\n",
    "\n",
    "grad_w1_theory_basic = W2.T @ (W2@W1@(square_m_basic*(X.T@X)) - X.T@(mean_m_basic*X)) * (2/m/n)\n",
    "grad_w2_theory_basic = (W2@W1@(square_m_basic*(X.T@X))-X.T@(mean_m_basic*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_basic = 0\n",
    "grad_w2_numer_basic = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_basic += torch.autograd.functional.jacobian(loss_func_W1_basic, W1)\n",
    "    grad_w2_numer_basic += torch.autograd.functional.jacobian(loss_func_W2_basic, W2)\n",
    "    #W1.detach()\n",
    "    #W2.detach()\n",
    "grad_w1_numer_basic /= N\n",
    "grad_w2_numer_basic /= N\n",
    "\n",
    "# absolute value check\n",
    "abs(grad_w1_theory_basic-grad_w1_numer_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.0019083305960521102\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.0018185757799074054\n"
     ]
    }
   ],
   "source": [
    "# matrix norm check\n",
    "norm_diff_w1_basic = torch.linalg.matrix_norm(grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "norm_diff_w2_basic = torch.linalg.matrix_norm(grad_w2_numer_basic-grad_w2_theory_basic)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_basic.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_basic.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[-0.0247,  0.0949,  0.0767,  0.0656],\n",
      "        [ 0.2116,  0.1435,  0.1816,  0.4203]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[ 0.2152,  0.1965],\n",
      "        [-0.0246, -0.1369],\n",
      "        [ 0.1903,  0.1562],\n",
      "        [ 0.1057, -0.0219]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[-0.0601,  0.0100, -0.0284, -0.0923],\n",
      "        [ 0.0426, -0.0940, -0.0192,  0.0660]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[ 0.0660, -0.0160],\n",
      "        [ 0.0389, -0.0427],\n",
      "        [ 0.0507, -0.0314],\n",
      "        [ 0.0580, -0.0558]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob, m, sample_dim, p)\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_basic = W20.T @ (W20@W10@(square_m_basic*(X.T@X)) - X.T@(mean_m_basic*X)) * (2/m/n)\n",
    "grad_w2_theory_basic = (W20@W10@(square_m_basic*(X.T@X))-X.T@(mean_m_basic*X)) @ W10.T * (2/m/n)\n",
    "\n",
    "# autograd gradients\n",
    "grad_w1_numer_basic = torch.autograd.functional.jacobian(loss_func_W1_basic, W10)\n",
    "grad_w2_numer_basic = torch.autograd.functional.jacobian(loss_func_W2_basic, W20)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking with different rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_probs(W1):\n",
    "    z = (mask_dropping_probs(prob_list, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_probs(W2):\n",
    "    z = (mask_dropping_probs(prob_list, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4836e-04, 1.3860e-03, 4.9757e-05, 1.9649e-03],\n",
       "        [7.2881e-04, 1.0648e-03, 2.2769e-05, 2.2316e-03]],\n",
       "       grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_dropping_probs\n",
    "#prob_list = torch.arange(1/(n+1), 1-0.01, 1/(n+1))\n",
    "#random.shuffle(prob_list)\n",
    "prob_list = torch.rand(n)*0.1 + 0.2\n",
    "mean_m_probs = prob_list.repeat(m, 1)\n",
    "square_m_probs = prob_list.view(n, 1) @ prob_list.view(1, n)\n",
    "square_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\n",
    "\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X)) * (2/m/n)\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_probs = 0\n",
    "grad_w2_numer_probs = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_probs += torch.autograd.functional.jacobian(loss_func_W1_probs, W1)\n",
    "    grad_w2_numer_probs += torch.autograd.functional.jacobian(loss_func_W2_probs, W2)\n",
    "grad_w1_numer_probs /= N\n",
    "grad_w2_numer_probs /= N\n",
    "\n",
    "abs(grad_w1_theory_probs-grad_w1_numer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.0036509279161691666\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.0023121535778045654\n"
     ]
    }
   ],
   "source": [
    "norm_diff_w1_probs = torch.linalg.matrix_norm(grad_w1_numer_probs-grad_w1_theory_probs)\n",
    "norm_diff_w2_probs = torch.linalg.matrix_norm(grad_w2_numer_probs-grad_w2_theory_probs)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_probs.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_probs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small mask test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nm = 1\\nn = 2\\nX = torch.tensor([[2, 3]]).float()\\nW1 = torch.tensor([[1, 0], [1, 1]]).float()\\nW2 = torch.tensor([[1, -1], [0, 1]]).float()\\n\\nprob_list = torch.tensor([0.5, 0.8])\\nmean_m_probs = prob_list.repeat(1, 1)\\nsquare_m_probs = prob_list.view(2, 1) @ prob_list.view(1, 2)\\nsquare_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\\n\\ngrad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X))\\ngrad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T\\n\\nprint(grad_w1_theory_probs)\\nprint(torch.autograd.functional.jacobian(loss_func_W1_probs, W1))\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "m = 1\n",
    "n = 2\n",
    "X = torch.tensor([[2, 3]]).float()\n",
    "W1 = torch.tensor([[1, 0], [1, 1]]).float()\n",
    "W2 = torch.tensor([[1, -1], [0, 1]]).float()\n",
    "\n",
    "prob_list = torch.tensor([0.5, 0.8])\n",
    "mean_m_probs = prob_list.repeat(1, 1)\n",
    "square_m_probs = prob_list.view(2, 1) @ prob_list.view(1, 2)\n",
    "square_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\n",
    "\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X))\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T\n",
    "\n",
    "print(grad_w1_theory_probs)\n",
    "print(torch.autograd.functional.jacobian(loss_func_W1_probs, W1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[0.0901, 0.1308, 0.1312, 0.2385],\n",
      "        [0.0398, 0.0421, 0.0918, 0.1560]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[ 0.1920,  0.3329],\n",
      "        [-0.1256,  0.0346],\n",
      "        [ 0.1294,  0.3111],\n",
      "        [ 0.0284,  0.1786]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[ 0.0547,  0.0459,  0.0260,  0.0806],\n",
      "        [-0.1292, -0.1954, -0.1091, -0.1984]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[ 0.0428,  0.1204],\n",
      "        [-0.0621,  0.1289],\n",
      "        [-0.0102,  0.1235],\n",
      "        [-0.0193,  0.1447]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob_list, m, sample_dim, p, type='probs')\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X)) * (2/m/n)\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T * (2/m/n)\n",
    "# autograd gradients\n",
    "grad_w1_numer_probs = torch.autograd.functional.jacobian(loss_func_W1_probs, W1)\n",
    "grad_w2_numer_probs = torch.autograd.functional.jacobian(loss_func_W2_probs, W2)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking in terms of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_patches(W1):\n",
    "    z = (mask_patches(prob, patch_size, m, sample_dim)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_patches(W2):\n",
    "    z = (mask_patches(prob, patch_size, m, sample_dim)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0003, 0.0006, 0.0003, 0.0005],\n",
       "        [0.0003, 0.0007, 0.0004, 0.0007]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_patches\n",
    "mean_m_patches = torch.ones(m, n) * prob\n",
    "patch_size = [2, 1]\n",
    "pix_num = torch.div(torch.tensor(sample_dim), torch.tensor(patch_size), rounding_mode='floor')\n",
    "mat_patches = torch.arange(pix_num[0]*pix_num[1]).view(*pix_num)\n",
    "mat_patches = torch.repeat_interleave(mat_patches, patch_size[1], dim=1)\n",
    "mat_patches = torch.repeat_interleave(mat_patches, patch_size[0], dim=0).view(n)\n",
    "square_m_patches = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if mat_patches[i] == mat_patches[j]:\n",
    "            square_m_patches[i, j] = prob\n",
    "        else:\n",
    "            square_m_patches[i, j] = prob**2\n",
    "\n",
    "grad_w1_theory_patches = W2.T @ (W2@W1@(square_m_patches*(X.T@X)) - X.T@(mean_m_patches*X)) * (2/m/n)\n",
    "grad_w2_theory_patches = (W2@W1@(square_m_patches*(X.T@X))-X.T@(mean_m_patches*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_patches = 0\n",
    "grad_w2_numer_patches = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_patches += torch.autograd.functional.jacobian(loss_func_W1_patches, W1)\n",
    "    grad_w2_numer_patches += torch.autograd.functional.jacobian(loss_func_W2_patches, W2)\n",
    "grad_w1_numer_patches /= N\n",
    "grad_w2_numer_patches /= N\n",
    "\n",
    "abs(grad_w1_theory_patches-grad_w1_numer_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.0014025537529960275\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.005967330187559128\n"
     ]
    }
   ],
   "source": [
    "norm_diff_w1_patches = torch.linalg.matrix_norm(grad_w1_numer_patches-grad_w1_theory_patches)\n",
    "norm_diff_w2_patches = torch.linalg.matrix_norm(grad_w2_numer_patches-grad_w2_theory_patches)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_patches.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_patches.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[ 0.0901, -0.0779,  0.0164,  0.0970],\n",
      "        [ 0.0398,  0.0037,  0.0617,  0.1137]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[ 0.1942,  0.3982],\n",
      "        [-0.0475,  0.0602],\n",
      "        [ 0.1653,  0.3566],\n",
      "        [ 0.0752,  0.2145]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[ 0.0547, -0.1628, -0.0888, -0.0608],\n",
      "        [-0.1292, -0.2338, -0.1392, -0.2407]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.0354, -0.0849, -0.1052, -0.1579],\n",
      "        [-0.1689, -0.2375, -0.2009, -0.3544]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[0.0450, 0.1857],\n",
      "        [0.0160, 0.1544],\n",
      "        [0.0257, 0.1690],\n",
      "        [0.0275, 0.1806]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob, m, sample_dim, p, type='patches', patch_size=[1, 1])\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_patches = W2.T @ (W2@W1@(square_m_patches*(X.T@X)) - X.T@(mean_m_patches*X)) * (2/m/n)\n",
    "grad_w2_theory_patches = (W2@W1@(square_m_patches*(X.T@X))-X.T@(mean_m_patches*X)) @ W1.T * (2/m/n)\n",
    "# autograd gradients\n",
    "grad_w1_numer_patches = torch.autograd.functional.jacobian(loss_func_W1_patches, W1)\n",
    "grad_w2_numer_patches = torch.autograd.functional.jacobian(loss_func_W2_patches, W2)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('SSL_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c3c12edae13c0508bae753e64eeaec91e16c05e32c8c48bac5bd7ee2e37cd7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
