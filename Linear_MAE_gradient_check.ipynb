{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Verify the gradient derivation of linear masked autoencoder.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise matrix and vector\n",
    "# prob is un-masking prob\n",
    "sample_num = 3\n",
    "H = 4\n",
    "W = 1\n",
    "sample_dim = [H, W]\n",
    "feature_num = H * W\n",
    "\n",
    "m = sample_num\n",
    "n = feature_num\n",
    "prob = 0.75\n",
    "X = torch.rand(m, n)\n",
    "W1 = torch.rand(n, n, requires_grad=True)\n",
    "W2 = torch.rand(n, n, requires_grad=True)\n",
    "#print(X)\n",
    "#print(W1)\n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different types of masks\n",
    "def mask_basic(prob, sample_num, feature_num):\n",
    "    return torch.zeros(sample_num, feature_num).bernoulli_(prob)\n",
    "\n",
    "def mask_dropping_probs(prob_list: torch.Tensor, sample_num, feature_num):\n",
    "    return torch.zeros(sample_num, feature_num).bernoulli_(prob_list)\n",
    "\n",
    "def mask_patches(prob, patch_size, sample_num, sample_dim):\n",
    "    patch_size = torch.tensor(patch_size)\n",
    "    sample_dim = torch.tensor(sample_dim)\n",
    "    feature_num = sample_dim[0]*sample_dim[1]\n",
    "    div_check = sample_dim % patch_size == torch.zeros(2)\n",
    "    if torch.all(div_check):\n",
    "        pix_num = torch.div(sample_dim, patch_size, rounding_mode='floor')\n",
    "        mat_patches = torch.zeros(sample_num, *pix_num).bernoulli_(prob)\n",
    "        mat_patches = torch.repeat_interleave(mat_patches, patch_size[1], dim=2)\n",
    "        return mat_patches.repeat_interleave(patch_size[0], dim=1).view(sample_num, feature_num)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Both height ({H}) and width ({W}) should be divisible by patch_size ({patch_size}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked autoencoder (linear)\n",
    "class M_LAE(nn.Module):\n",
    "    def __init__(self, prob, sample_num, sample_dim, type='basic', patch_size=None):\n",
    "        super(M_LAE, self).__init__()\n",
    "        self.prob = prob\n",
    "        self.m = sample_num\n",
    "        self.sample_dim = sample_dim\n",
    "        self.H, self.W = sample_dim\n",
    "        self.n = self.H * self.W\n",
    "        if type not in ['basic', 'probs', 'patches']:\n",
    "            raise NotImplementedError(\"Could only implement 'basic', 'probs' and 'patches' type of masking.\")\n",
    "        else:\n",
    "            self.masking_type = type\n",
    "        if patch_size is not None:\n",
    "            self.patch_size = patch_size\n",
    "        w1 = nn.Linear(self.n, self.n, bias=False)\n",
    "        w2 = nn.Linear(self.n, self.n, bias=False)\n",
    "        self.body = nn.Sequential(*[w1, w2])\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        if mask is None:\n",
    "            if self.masking_type == 'basic':\n",
    "                mask = mask_basic(self.prob, self.m, self.n)\n",
    "            elif self.masking_type == 'probs':\n",
    "                mask = mask_dropping_probs(self.prob, self.m, self.n)\n",
    "            elif self.masking_type == 'patches':\n",
    "                mask = mask_patches(self.prob, self.patch_size, self.m, self.sample_dim)\n",
    "        Y = mask * X\n",
    "        Y = self.body(Y)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_basic(W1):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_basic(W2):\n",
    "    z = (mask_basic(prob, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0040, 0.0010, 0.0056, 0.0029],\n",
       "        [0.0043, 0.0010, 0.0055, 0.0030],\n",
       "        [0.0036, 0.0008, 0.0048, 0.0025],\n",
       "        [0.0047, 0.0011, 0.0071, 0.0036]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the theoretical and numerical solutions of W1 and W2\n",
    "# mask_basic\n",
    "mean_m_basic = torch.ones(m, n) * prob\n",
    "square_m_basic = torch.ones(n, n) * prob**2\n",
    "square_m_basic.fill_diagonal_(prob)\n",
    "\n",
    "grad_w1_theory_basic = W2.T @ (W2@W1@(square_m_basic*(X.T@X)) - X.T@(mean_m_basic*X)) * (2/m/n)\n",
    "grad_w2_theory_basic = (W2@W1@(square_m_basic*(X.T@X))-X.T@(mean_m_basic*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_basic = 0\n",
    "grad_w2_numer_basic = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_basic += torch.autograd.functional.jacobian(loss_func_W1_basic, W1)\n",
    "    grad_w2_numer_basic += torch.autograd.functional.jacobian(loss_func_W2_basic, W2)\n",
    "    #W1.detach()\n",
    "    #W2.detach()\n",
    "grad_w1_numer_basic /= N\n",
    "grad_w2_numer_basic /= N\n",
    "\n",
    "# absolute value check\n",
    "abs(grad_w1_theory_basic-grad_w1_numer_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.015652021393179893\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.00986055750399828\n"
     ]
    }
   ],
   "source": [
    "# matrix norm check\n",
    "norm_diff_w1_basic = torch.linalg.matrix_norm(grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "norm_diff_w2_basic = torch.linalg.matrix_norm(grad_w2_numer_basic-grad_w2_theory_basic)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_basic.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_basic.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[0.7996, 0.1777, 0.3846, 0.4695],\n",
      "        [0.6469, 0.1431, 0.4264, 0.4093],\n",
      "        [0.5162, 0.1140, 0.1997, 0.2960],\n",
      "        [0.9573, 0.1967, 0.5212, 0.5788]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[0.7163, 0.7473, 1.0426, 0.5126],\n",
      "        [0.6173, 0.6946, 0.8368, 0.3907],\n",
      "        [0.4021, 0.4329, 0.6343, 0.2957],\n",
      "        [0.1292, 0.0326, 0.3160, 0.1945]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[-0.0459, -0.0321, -0.0110,  0.0003],\n",
      "        [-0.0692, -0.0628,  0.0597, -0.0058],\n",
      "        [ 0.0338,  0.0223,  0.0245,  0.0028],\n",
      "        [-0.0388, -0.0266,  0.0416,  0.0217]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[-0.0461,  0.0148, -0.0286, -0.0531],\n",
      "        [-0.0259, -0.0046, -0.0169, -0.0257],\n",
      "        [-0.0349, -0.0031, -0.0817, -0.0510],\n",
      "        [-0.0305,  0.0361, -0.0379, -0.0484]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob, m, sample_dim)\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_basic = W20.T @ (W20@W10@(square_m_basic*(X.T@X)) - X.T@(mean_m_basic*X)) * (2/m/n)\n",
    "grad_w2_theory_basic = (W20@W10@(square_m_basic*(X.T@X))-X.T@(mean_m_basic*X)) @ W10.T * (2/m/n)\n",
    "\n",
    "# autograd gradients\n",
    "grad_w1_numer_basic = torch.autograd.functional.jacobian(loss_func_W1_basic, W10)\n",
    "grad_w2_numer_basic = torch.autograd.functional.jacobian(loss_func_W2_basic, W20)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking with different rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_probs(W1):\n",
    "    z = (mask_dropping_probs(prob_list, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_probs(W2):\n",
    "    z = (mask_dropping_probs(prob_list, m, n)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0028, 0.0002, 0.0015, 0.0004],\n",
       "        [0.0028, 0.0003, 0.0014, 0.0002],\n",
       "        [0.0024, 0.0001, 0.0012, 0.0004],\n",
       "        [0.0038, 0.0003, 0.0019, 0.0006]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_dropping_probs\n",
    "#prob_list = torch.arange(1/(n+1), 1-0.01, 1/(n+1))\n",
    "#random.shuffle(prob_list)\n",
    "prob_list = torch.rand(n)*0.1 + 0.2\n",
    "mean_m_probs = prob_list.repeat(m, 1)\n",
    "square_m_probs = prob_list.view(n, 1) @ prob_list.view(1, n)\n",
    "square_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\n",
    "\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X)) * (2/m/n)\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_probs = 0\n",
    "grad_w2_numer_probs = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_probs += torch.autograd.functional.jacobian(loss_func_W1_probs, W1)\n",
    "    grad_w2_numer_probs += torch.autograd.functional.jacobian(loss_func_W2_probs, W2)\n",
    "grad_w1_numer_probs /= N\n",
    "grad_w2_numer_probs /= N\n",
    "\n",
    "abs(grad_w1_theory_probs-grad_w1_numer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.006764405872672796\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.001338590169325471\n"
     ]
    }
   ],
   "source": [
    "norm_diff_w1_probs = torch.linalg.matrix_norm(grad_w1_numer_probs-grad_w1_theory_probs)\n",
    "norm_diff_w2_probs = torch.linalg.matrix_norm(grad_w2_numer_probs-grad_w2_theory_probs)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_probs.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_probs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small mask test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nm = 1\\nn = 2\\nX = torch.tensor([[2, 3]]).float()\\nW1 = torch.tensor([[1, 0], [1, 1]]).float()\\nW2 = torch.tensor([[1, -1], [0, 1]]).float()\\n\\nprob_list = torch.tensor([0.5, 0.8])\\nmean_m_probs = prob_list.repeat(1, 1)\\nsquare_m_probs = prob_list.view(2, 1) @ prob_list.view(1, 2)\\nsquare_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\\n\\ngrad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X))\\ngrad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T\\n\\nprint(grad_w1_theory_probs)\\nprint(torch.autograd.functional.jacobian(loss_func_W1_probs, W1))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "m = 1\n",
    "n = 2\n",
    "X = torch.tensor([[2, 3]]).float()\n",
    "W1 = torch.tensor([[1, 0], [1, 1]]).float()\n",
    "W2 = torch.tensor([[1, -1], [0, 1]]).float()\n",
    "\n",
    "prob_list = torch.tensor([0.5, 0.8])\n",
    "mean_m_probs = prob_list.repeat(1, 1)\n",
    "square_m_probs = prob_list.view(2, 1) @ prob_list.view(1, 2)\n",
    "square_m_probs = square_m_probs.fill_diagonal_(0) + torch.diag(prob_list)\n",
    "\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X))\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T\n",
    "\n",
    "print(grad_w1_theory_probs)\n",
    "print(torch.autograd.functional.jacobian(loss_func_W1_probs, W1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[0.7237, 0.1594, 0.3120, 0.4054],\n",
      "        [0.4711, 0.1159, 0.2449, 0.2504],\n",
      "        [0.4813, 0.1316, 0.2401, 0.3043],\n",
      "        [0.9083, 0.2050, 0.4371, 0.4898]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[0.7400, 0.8676, 0.9506, 0.4633],\n",
      "        [0.6329, 0.7573, 0.8233, 0.4025],\n",
      "        [0.4337, 0.5606, 0.5946, 0.2979],\n",
      "        [0.1425, 0.1463, 0.1684, 0.0825]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[-0.1218, -0.0504, -0.0836, -0.0638],\n",
      "        [-0.2449, -0.0900, -0.1218, -0.1646],\n",
      "        [-0.0011,  0.0399,  0.0649,  0.0111],\n",
      "        [-0.0877, -0.0183, -0.0425, -0.0673]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[-0.0224,  0.1352, -0.1206, -0.1025],\n",
      "        [-0.0102,  0.0580, -0.0305, -0.0139],\n",
      "        [-0.0033,  0.1245, -0.1214, -0.0488],\n",
      "        [-0.0173,  0.1498, -0.1856, -0.1604]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob_list, m, sample_dim, type='probs')\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_probs = W2.T @ (W2@W1@(square_m_probs*(X.T@X)) - X.T@(mean_m_probs*X)) * (2/m/n)\n",
    "grad_w2_theory_probs = (W2@W1@(square_m_probs*(X.T@X))-X.T@(mean_m_probs*X)) @ W1.T * (2/m/n)\n",
    "# autograd gradients\n",
    "grad_w1_numer_probs = torch.autograd.functional.jacobian(loss_func_W1_probs, W1)\n",
    "grad_w2_numer_probs = torch.autograd.functional.jacobian(loss_func_W2_probs, W2)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking in terms of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function in terms of W1 and W2\n",
    "def loss_func_W1_patches(W1):\n",
    "    z = (mask_patches(prob, patch_size, m, sample_dim)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n\n",
    "\n",
    "def loss_func_W2_patches(W2):\n",
    "    z = (mask_patches(prob, patch_size, m, sample_dim)*X) @ W1.T @ W2.T - X\n",
    "    return sum(sum(z*z)) / m / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0029, 0.0012, 0.0004, 0.0020],\n",
       "        [0.0029, 0.0011, 0.0004, 0.0020],\n",
       "        [0.0026, 0.0011, 0.0004, 0.0016],\n",
       "        [0.0039, 0.0016, 0.0004, 0.0027]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_patches\n",
    "mean_m_patches = torch.ones(m, n) * prob\n",
    "patch_size = [2, 1]\n",
    "pix_num = torch.div(torch.tensor(sample_dim), torch.tensor(patch_size), rounding_mode='floor')\n",
    "mat_patches = torch.arange(pix_num[0]*pix_num[1]).view(*pix_num)\n",
    "mat_patches = torch.repeat_interleave(mat_patches, patch_size[1], dim=1)\n",
    "mat_patches = torch.repeat_interleave(mat_patches, patch_size[0], dim=0).view(n)\n",
    "square_m_patches = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if mat_patches[i] == mat_patches[j]:\n",
    "            square_m_patches[i, j] = prob\n",
    "        else:\n",
    "            square_m_patches[i, j] = prob**2\n",
    "\n",
    "grad_w1_theory_patches = W2.T @ (W2@W1@(square_m_patches*(X.T@X)) - X.T@(mean_m_patches*X)) * (2/m/n)\n",
    "grad_w2_theory_patches = (W2@W1@(square_m_patches*(X.T@X))-X.T@(mean_m_patches*X)) @ W1.T * (2/m/n)\n",
    "\n",
    "# Initialising sampling\n",
    "N = 10000\n",
    "grad_w1_numer_patches = 0\n",
    "grad_w2_numer_patches = 0\n",
    "# Sampling process\n",
    "for i in range(N):\n",
    "    grad_w1_numer_patches += torch.autograd.functional.jacobian(loss_func_W1_patches, W1)\n",
    "    grad_w2_numer_patches += torch.autograd.functional.jacobian(loss_func_W2_patches, W2)\n",
    "grad_w1_numer_patches /= N\n",
    "grad_w2_numer_patches /= N\n",
    "\n",
    "abs(grad_w1_theory_patches-grad_w1_numer_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix norm of the difference between theoretical and numerical solutions of W1: 0.007966650649905205\n",
      "The matrix norm of the difference between theoretical and numerical solutions of W2: 0.0035006017424166203\n"
     ]
    }
   ],
   "source": [
    "norm_diff_w1_patches = torch.linalg.matrix_norm(grad_w1_numer_patches-grad_w1_theory_patches)\n",
    "norm_diff_w2_patches = torch.linalg.matrix_norm(grad_w2_numer_patches-grad_w2_theory_patches)\n",
    "\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W1:\", norm_diff_w1_patches.item())\n",
    "print(\"The matrix norm of the difference between theoretical and numerical solutions of W2:\", norm_diff_w2_patches.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between autograd and gradient decent for w1\n",
      " tensor([[0.6982, 0.1474, 0.3014, 0.3718],\n",
      "        [0.5052, 0.0978, 0.2262, 0.2680],\n",
      "        [0.7942, 0.2380, 0.3881, 0.4633],\n",
      "        [0.7188, 0.0699, 0.2477, 0.3994]])\n",
      "The difference between autograd and gradient decent for w2\n",
      " tensor([[0.6646, 0.9037, 0.8835, 0.5673],\n",
      "        [0.5958, 0.7679, 0.7840, 0.4498],\n",
      "        [0.3888, 0.5327, 0.5511, 0.3683],\n",
      "        [0.1078, 0.1927, 0.1291, 0.1451]])\n",
      "difference in terms of autograd for w1\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w1\n",
      " tensor([[-0.1473, -0.0624, -0.0942, -0.0974],\n",
      "        [-0.2109, -0.1080, -0.1405, -0.1470],\n",
      "        [ 0.3118,  0.1463,  0.2129,  0.1701],\n",
      "        [-0.2772, -0.1533, -0.2319, -0.1577]])\n",
      "difference in terms of autograd for w2\n",
      " tensor([[-0.8455, -0.2098, -0.3956, -0.4692],\n",
      "        [-0.7161, -0.2059, -0.3667, -0.4150],\n",
      "        [-0.4824, -0.0917, -0.1752, -0.2933],\n",
      "        [-0.9960, -0.2233, -0.4796, -0.5571]])\n",
      "difference in terms of gradient decent for w2\n",
      " tensor([[-0.0979,  0.1712, -0.1877,  0.0015],\n",
      "        [-0.0473,  0.0687, -0.0698,  0.0335],\n",
      "        [-0.0482,  0.0966, -0.1648,  0.0216],\n",
      "        [-0.0520,  0.1961, -0.2248, -0.0978]])\n"
     ]
    }
   ],
   "source": [
    "# Initialising network\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "m_net = M_LAE(prob, m, sample_dim, type='patches', patch_size=[1, 1])\n",
    "inputs = X\n",
    "targets = X\n",
    "\n",
    "optimizer = optim.SGD(m_net.body.parameters(), lr=learning_rate)\n",
    "params0 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W10 = params0[0].clone().detach()\n",
    "W20 = params0[1].clone().detach()\n",
    "\n",
    "# theoretical gradients\n",
    "grad_w1_theory_patches = W2.T @ (W2@W1@(square_m_patches*(X.T@X)) - X.T@(mean_m_patches*X)) * (2/m/n)\n",
    "grad_w2_theory_patches = (W2@W1@(square_m_patches*(X.T@X))-X.T@(mean_m_patches*X)) @ W1.T * (2/m/n)\n",
    "# autograd gradients\n",
    "grad_w1_numer_patches = torch.autograd.functional.jacobian(loss_func_W1_patches, W1)\n",
    "grad_w2_numer_patches = torch.autograd.functional.jacobian(loss_func_W2_patches, W2)\n",
    "\n",
    "# one-step gradient decent\n",
    "optimizer.zero_grad()\n",
    "outputs = m_net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params1 = list(m_net.body.parameters())\n",
    "#print(list(m_net.body.parameters()))\n",
    "W11 = params1[0].clone().detach()\n",
    "W21 = params1[1].clone().detach()\n",
    "\n",
    "gradient_W1 = (W10 - W11) / learning_rate\n",
    "gradient_W2 = (W20 - W21) / learning_rate\n",
    "print('The difference between autograd and gradient decent for w1\\n', gradient_W1-grad_w1_numer_basic)\n",
    "print('The difference between autograd and gradient decent for w2\\n', gradient_W2-grad_w2_numer_basic)\n",
    "print('difference in terms of autograd for w1\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w1\\n', gradient_W1-grad_w1_theory_basic)\n",
    "print('difference in terms of autograd for w2\\n', grad_w1_numer_basic-grad_w1_theory_basic)\n",
    "print('difference in terms of gradient decent for w2\\n', gradient_W2-grad_w2_theory_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('SSL_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c3c12edae13c0508bae753e64eeaec91e16c05e32c8c48bac5bd7ee2e37cd7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
